{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d6b8a7",
   "metadata": {},
   "source": [
    "# 第二模型训练（回归/分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44074895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc60c9c",
   "metadata": {},
   "source": [
    "模型路径：\n",
    "1. 通过简单的决策树分类出确定的0\n",
    "2. 然后通过mlp对较为均衡的数据集再进行学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d3cd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 MLP 模型\n",
    "def build_mlp_ecg(input_length, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_length,)),\n",
    "\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae14708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d6e9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data frame\n",
    "# === 加载 MIT-BIH (5类) ===\n",
    "mitbih_test = pd.read_csv(\"../data/ecg_category/mitbih_test.csv\")\n",
    "mitbih_train = pd.read_csv(\"../data/ecg_category/mitbih_train.csv\")\n",
    "\n",
    "X_mitbih_train, y_mitbih_train = mitbih_train.iloc[:, :-1].values, mitbih_train.iloc[:, -1].values\n",
    "X_mitbih_test, y_mitbih_test = mitbih_test.iloc[:, :-1].values, mitbih_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c91e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集标签分布：\n",
      "0.0    72470\n",
      "4.0     6431\n",
      "2.0     5788\n",
      "1.0     2223\n",
      "3.0      641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集标签分布：\n",
      "0.0    18117\n",
      "4.0     1608\n",
      "2.0     1448\n",
      "1.0      556\n",
      "3.0      162\n",
      "Name: count, dtype: int64\n",
      "\n",
      "y_mitbih_train dtype: float64\n",
      "y_mitbih_train unique values: [0. 1. 2. 3. 4.]\n",
      "y_mitbih_test unique values: [0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# === 检查标签分布 ===\n",
    "print(\"训练集标签分布：\")\n",
    "print(pd.Series(y_mitbih_train).value_counts())\n",
    "\n",
    "print(\"\\n测试集标签分布：\")\n",
    "print(pd.Series(y_mitbih_test).value_counts())\n",
    "\n",
    "# === 检查标签类型 ===\n",
    "print(\"\\ny_mitbih_train dtype:\", y_mitbih_train.dtype)\n",
    "print(\"y_mitbih_train unique values:\", np.unique(y_mitbih_train)[:10])\n",
    "print(\"y_mitbih_test unique values:\", np.unique(y_mitbih_test)[:10])\n",
    "\n",
    "# === 强制转 int，避免 float/NaN ===\n",
    "y_mitbih_train = y_mitbih_train.astype(int)\n",
    "y_mitbih_test = y_mitbih_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7e49c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原训练集大小: 87553 筛后训练集大小: 24479\n",
      "筛后类别分布: {0: 9396, 1: 2223, 2: 5788, 3: 641, 4: 6431}\n"
     ]
    }
   ],
   "source": [
    "# 1) 训练集上用决策树筛“易例 0”\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 假设已有 X_train, y_train (标签为 0..4 的整数)\n",
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=21,            # 浅一点，避免过拟合\n",
    "    min_samples_leaf=20,    # 提高叶子样本量，稳一些\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "tree.fit(X_mitbih_train, y_mitbih_train)\n",
    "\n",
    "# 预测 P(y=0|x)\n",
    "p0 = tree.predict_proba(X_mitbih_train)[:, 0]\n",
    "\n",
    "tau = 0.95  # 训练阶段阈值（可用验证集调参）\n",
    "is_easy_zero = (y_mitbih_train == 0) & (p0 >= tau)\n",
    "\n",
    "# 可选：限制最多剔除的数量，避免过头\n",
    "easy_zero_idx = np.where(is_easy_zero)[0]\n",
    "\n",
    "# 构建“难例训练集”：保留所有少数类 + 难例0\n",
    "keep_mask = np.ones_like(y_mitbih_train, dtype=bool)\n",
    "keep_mask[easy_zero_idx] = False\n",
    "X_train_hard = X_mitbih_train[keep_mask]\n",
    "y_train_hard = y_mitbih_train[keep_mask]\n",
    "\n",
    "print(\"原训练集大小:\", len(y_mitbih_train), \"筛后训练集大小:\", len(y_train_hard))\n",
    "unique, counts = np.unique(y_train_hard, return_counts=True)\n",
    "print(\"筛后类别分布:\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cee03ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_ = [0 1 2 3 4]\n",
      "p0 quantiles: [0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "p0 | y=0 quantiles: [1.13737725e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "tau=0.95: 训练集里 高p0比例，总=0.7205, 真0占比=0.7204, 误杀非0占比=0.0001\n",
      "tau=0.98: 训练集里 高p0比例，总=0.6881, 真0占比=0.6881, 误杀非0占比=0.0000\n",
      "tau=0.99: 训练集里 高p0比例，总=0.6881, 真0占比=0.6881, 误杀非0占比=0.0000\n"
     ]
    }
   ],
   "source": [
    "# 1) 确认 predict_proba 列顺序\n",
    "print(\"classes_ =\", tree.classes_)   # 应为 [0,1,2,3,4]\n",
    "p = tree.predict_proba(X_mitbih_train)      # shape: (N, 5)\n",
    "p0 = p[:, list(tree.classes_).index(0)]\n",
    "\n",
    "# 2) 看一下 p0 的分布\n",
    "import numpy as np\n",
    "q = np.quantile(p0, [0, .5, .9, .95, .98, .99, .995, 1.0])\n",
    "print(\"p0 quantiles:\", q)\n",
    "\n",
    "# 3) 看一下在真0类里的 p0\n",
    "q0 = np.quantile(p0[y_mitbih_train==0], [0, .5, .9, .95, .98, .99, .995, 1.0])\n",
    "print(\"p0 | y=0 quantiles:\", q0)\n",
    "\n",
    "# 4) 也看一下非0类里被“高p0”覆盖多少\n",
    "for t in [0.95, 0.98, 0.99]:\n",
    "    tp = np.mean((p0>=t) & (y_mitbih_train==0))\n",
    "    fp = np.mean((p0>=t) & (y_mitbih_train!=0))\n",
    "    print(f\"tau={t}: 训练集里 高p0比例，总={np.mean(p0>=t):.4f}, 真0占比={tp:.4f}, 误杀非0占比={fp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f95129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - accuracy: 0.5242 - loss: 1.1204 - val_accuracy: 0.6473 - val_loss: 1.0389 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5835 - loss: 0.9466 - val_accuracy: 0.6800 - val_loss: 0.9272 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5867 - loss: 0.9131 - val_accuracy: 0.6593 - val_loss: 0.8837 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5983 - loss: 0.8776 - val_accuracy: 0.6408 - val_loss: 0.9118 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6068 - loss: 0.8643 - val_accuracy: 0.6035 - val_loss: 0.9612 - learning_rate: 0.0010\n",
      "Epoch 6/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6015 - loss: 0.8514 - val_accuracy: 0.6176 - val_loss: 0.9355 - learning_rate: 0.0010\n",
      "Epoch 7/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6130 - loss: 0.8314 - val_accuracy: 0.6266 - val_loss: 0.9209 - learning_rate: 5.0000e-04\n",
      "Epoch 8/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6176 - loss: 0.8375 - val_accuracy: 0.6525 - val_loss: 0.8646 - learning_rate: 5.0000e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6183 - loss: 0.8271 - val_accuracy: 0.6291 - val_loss: 0.8970 - learning_rate: 5.0000e-04\n",
      "Epoch 10/10\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6174 - loss: 0.8274 - val_accuracy: 0.6492 - val_loss: 0.8738 - learning_rate: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x358538e50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class_weight（基于筛后训练集）\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cls_w = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(5),\n",
    "    y=y_train_hard\n",
    ")\n",
    "cls_w = {i: w for i, w in enumerate(cls_w)}\n",
    "\n",
    "# 一个稳健的 MLP\n",
    "import tensorflow as tf\n",
    "inputs = tf.keras.Input(shape=(X_train_hard.shape[1],))\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "mlp = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "mlp.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cb = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "]\n",
    "\n",
    "# 假设你已经得到筛后的：\n",
    "# X_train_hard, y_train_hard\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_hard, y_train_hard,\n",
    "    test_size=0.15,           # 15% 做验证\n",
    "    random_state=42,\n",
    "    stratify=y_train_hard     # 保持类别比例\n",
    ")\n",
    "\n",
    "# 然后把 fit 里的变量换成：\n",
    "mlp.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10, batch_size=256,\n",
    "    class_weight=cls_w,\n",
    "    callbacks=cb,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e2e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Val accuracy: 0.6525\n",
      "[MLP] Val accuracy(sklearn): 0.6525\n",
      "[MLP] Val classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7189    0.4393    0.5454      1409\n",
      "           1     0.4246    0.7246    0.5354       334\n",
      "           2     0.6806    0.6947    0.6876       868\n",
      "           3     0.1894    0.8229    0.3080        96\n",
      "           4     0.9094    0.8839    0.8965       965\n",
      "\n",
      "    accuracy                         0.6525      3672\n",
      "   macro avg     0.5846    0.7131    0.5946      3672\n",
      "weighted avg     0.7193    0.6525    0.6641      3672\n",
      "\n",
      "[MLP] Val confusion matrix:\n",
      " [[619 296 211 236  47]\n",
      " [ 63 242  15  12   2]\n",
      " [118  26 603  86  35]\n",
      " [ 11   2   3  79   1]\n",
      " [ 50   4  54   4 853]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1) Keras 自带 evaluate（只看准确率）\n",
    "val_loss, val_acc = mlp.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"[MLP] Val accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# 2) 更详细的指标\n",
    "y_val_pred = np.argmax(mlp.predict(X_val, verbose=0), axis=1)\n",
    "print(f\"[MLP] Val accuracy(sklearn): {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(\"[MLP] Val classification report:\\n\", classification_report(y_val, y_val_pred, digits=4))\n",
    "print(\"[MLP] Val confusion matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d16ee969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cascade@tau=0.95] acc=0.6520  macroF1=0.5942  gated=3/3672 (0.1%)  gate_fp=3 (0.13% of non-0)\n",
      "[Cascade@tau=0.97] acc=0.6520  macroF1=0.5942  gated=3/3672 (0.1%)  gate_fp=3 (0.13% of non-0)\n",
      "[Cascade@tau=0.98] acc=0.6525  macroF1=0.5946  gated=0/3672 (0.0%)  gate_fp=0 (0.00% of non-0)\n",
      "[Cascade@tau=0.99] acc=0.6525  macroF1=0.5946  gated=0/3672 (0.0%)  gate_fp=0 (0.00% of non-0)\n",
      "\n",
      "Best tau on val: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7189    0.4393    0.5454      1409\n",
      "           1     0.4246    0.7246    0.5354       334\n",
      "           2     0.6806    0.6947    0.6876       868\n",
      "           3     0.1894    0.8229    0.3080        96\n",
      "           4     0.9094    0.8839    0.8965       965\n",
      "\n",
      "    accuracy                         0.6525      3672\n",
      "   macro avg     0.5846    0.7131    0.5946      3672\n",
      "weighted avg     0.7193    0.6525    0.6641      3672\n",
      "\n",
      "[[619 296 211 236  47]\n",
      " [ 63 242  15  12   2]\n",
      " [118  26 603  86  35]\n",
      " [ 11   2   3  79   1]\n",
      " [ 50   4  54   4 853]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# 若你用的是多分类树（classes_=[0 1 2 3 4]），取 p0 的函数\n",
    "idx0 = list(tree.classes_).index(0)\n",
    "def p0_from_tree(tree, X):\n",
    "    return tree.predict_proba(X)[:, idx0]\n",
    "\n",
    "def predict_cascade_on(X, tau=0.98):\n",
    "    p0 = p0_from_tree(tree, X)\n",
    "    gate = (p0 >= tau)               # 高置信 0 → 直接判 0\n",
    "    y_pred = np.empty(X.shape[0], dtype=int)\n",
    "    idx = np.where(~gate)[0]         # 其余交给 MLP 五分类\n",
    "    if len(idx) > 0:\n",
    "        y_pred[idx] = np.argmax(mlp.predict(X[idx], verbose=0), axis=1)\n",
    "    y_pred[gate] = 0\n",
    "    return y_pred, gate\n",
    "\n",
    "taus = [0.95, 0.97, 0.98, 0.99]\n",
    "p0_val = p0_from_tree(tree, X_val)\n",
    "non0_val = np.sum(y_val != 0)\n",
    "\n",
    "best = None\n",
    "for tau in taus:\n",
    "    y_pred_c, gate = predict_cascade_on(X_val, tau=tau)\n",
    "    acc = accuracy_score(y_val, y_pred_c)\n",
    "    macro_f1 = f1_score(y_val, y_pred_c, average='macro')\n",
    "    gate_fp = np.sum(gate & (y_val != 0))  # 被树误杀的非0\n",
    "    print(f\"[Cascade@tau={tau}] acc={acc:.4f}  macroF1={macro_f1:.4f}  \"\n",
    "          f\"gated={gate.sum()}/{len(y_val)} ({gate.mean():.1%})  \"\n",
    "          f\"gate_fp={gate_fp} ({gate_fp/non0_val:.2%} of non-0)\")\n",
    "    score = (macro_f1, acc)  # 先看宏F1，再看acc\n",
    "    if (best is None) or (score > best[0]):\n",
    "        best = (score, tau, y_pred_c)\n",
    "\n",
    "best_tau = best[1]\n",
    "print(\"\\nBest tau on val:\", best_tau)\n",
    "print(classification_report(y_val, best[2], digits=4))\n",
    "print(confusion_matrix(y_val, best[2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
