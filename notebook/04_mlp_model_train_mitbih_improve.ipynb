{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d6b8a7",
   "metadata": {},
   "source": [
    "# 第二模型训练（回归/分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44074895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc60c9c",
   "metadata": {},
   "source": [
    "模型路径：\n",
    "1. 通过简单的决策树分类出确定的0\n",
    "2. 然后通过mlp对较为均衡的数据集再进行学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d3cd491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建 MLP 模型\n",
    "def build_mlp_ecg(input_length, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_length,)),\n",
    "\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.4),\n",
    "\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae14708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d6e9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data frame\n",
    "# === 加载 MIT-BIH (5类) ===\n",
    "mitbih_test = pd.read_csv(\"../data/ecg_category/mitbih_test.csv\")\n",
    "mitbih_train = pd.read_csv(\"../data/ecg_category/mitbih_train.csv\")\n",
    "\n",
    "X_mitbih_train, y_mitbih_train = mitbih_train.iloc[:, :-1].values, mitbih_train.iloc[:, -1].values\n",
    "X_mitbih_test, y_mitbih_test = mitbih_test.iloc[:, :-1].values, mitbih_test.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c91e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集标签分布：\n",
      "0.0    72470\n",
      "4.0     6431\n",
      "2.0     5788\n",
      "1.0     2223\n",
      "3.0      641\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集标签分布：\n",
      "0.0    18117\n",
      "4.0     1608\n",
      "2.0     1448\n",
      "1.0      556\n",
      "3.0      162\n",
      "Name: count, dtype: int64\n",
      "\n",
      "y_mitbih_train dtype: float64\n",
      "y_mitbih_train unique values: [0. 1. 2. 3. 4.]\n",
      "y_mitbih_test unique values: [0. 1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# === 检查标签分布 ===\n",
    "print(\"训练集标签分布：\")\n",
    "print(pd.Series(y_mitbih_train).value_counts())\n",
    "\n",
    "print(\"\\n测试集标签分布：\")\n",
    "print(pd.Series(y_mitbih_test).value_counts())\n",
    "\n",
    "# === 检查标签类型 ===\n",
    "print(\"\\ny_mitbih_train dtype:\", y_mitbih_train.dtype)\n",
    "print(\"y_mitbih_train unique values:\", np.unique(y_mitbih_train)[:10])\n",
    "print(\"y_mitbih_test unique values:\", np.unique(y_mitbih_test)[:10])\n",
    "\n",
    "# === 强制转 int，避免 float/NaN ===\n",
    "y_mitbih_train = y_mitbih_train.astype(int)\n",
    "y_mitbih_test = y_mitbih_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c7e49c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原训练集大小: 87553 筛后训练集大小: 24479\n",
      "筛后类别分布: {0: 9396, 1: 2223, 2: 5788, 3: 641, 4: 6431}\n"
     ]
    }
   ],
   "source": [
    "# 1) 训练集上用决策树筛“易例 0”\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 假设已有 X_train, y_train (标签为 0..4 的整数)\n",
    "tree = DecisionTreeClassifier(\n",
    "    max_depth=21,            # 浅一点，避免过拟合\n",
    "    min_samples_leaf=20,    # 提高叶子样本量，稳一些\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "tree.fit(X_mitbih_train, y_mitbih_train)\n",
    "\n",
    "# 预测 P(y=0|x)\n",
    "p0 = tree.predict_proba(X_mitbih_train)[:, 0]\n",
    "\n",
    "tau = 0.95  # 训练阶段阈值（可用验证集调参）\n",
    "is_easy_zero = (y_mitbih_train == 0) & (p0 >= tau)\n",
    "\n",
    "# 可选：限制最多剔除的数量，避免过头\n",
    "easy_zero_idx = np.where(is_easy_zero)[0]\n",
    "\n",
    "# 构建“难例训练集”：保留所有少数类 + 难例0\n",
    "keep_mask = np.ones_like(y_mitbih_train, dtype=bool)\n",
    "keep_mask[easy_zero_idx] = False\n",
    "X_train_hard = X_mitbih_train[keep_mask]\n",
    "y_train_hard = y_mitbih_train[keep_mask]\n",
    "\n",
    "print(\"原训练集大小:\", len(y_mitbih_train), \"筛后训练集大小:\", len(y_train_hard))\n",
    "unique, counts = np.unique(y_train_hard, return_counts=True)\n",
    "print(\"筛后类别分布:\", dict(zip(unique, counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cee03ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_ = [0 1 2 3 4]\n",
      "p0 quantiles: [0. 1. 1. 1. 1. 1. 1. 1.]\n",
      "p0 | y=0 quantiles: [1.13737725e-04 1.00000000e+00 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00]\n",
      "tau=0.95: 训练集里 高p0比例，总=0.7205, 真0占比=0.7204, 误杀非0占比=0.0001\n",
      "tau=0.98: 训练集里 高p0比例，总=0.6881, 真0占比=0.6881, 误杀非0占比=0.0000\n",
      "tau=0.99: 训练集里 高p0比例，总=0.6881, 真0占比=0.6881, 误杀非0占比=0.0000\n"
     ]
    }
   ],
   "source": [
    "# 1) 确认 predict_proba 列顺序\n",
    "print(\"classes_ =\", tree.classes_)   # 应为 [0,1,2,3,4]\n",
    "p = tree.predict_proba(X_mitbih_train)      # shape: (N, 5)\n",
    "p0 = p[:, list(tree.classes_).index(0)]\n",
    "\n",
    "# 2) 看一下 p0 的分布\n",
    "import numpy as np\n",
    "q = np.quantile(p0, [0, .5, .9, .95, .98, .99, .995, 1.0])\n",
    "print(\"p0 quantiles:\", q)\n",
    "\n",
    "# 3) 看一下在真0类里的 p0\n",
    "q0 = np.quantile(p0[y_mitbih_train==0], [0, .5, .9, .95, .98, .99, .995, 1.0])\n",
    "print(\"p0 | y=0 quantiles:\", q0)\n",
    "\n",
    "# 4) 也看一下非0类里被“高p0”覆盖多少\n",
    "for t in [0.95, 0.98, 0.99]:\n",
    "    tp = np.mean((p0>=t) & (y_mitbih_train==0))\n",
    "    fp = np.mean((p0>=t) & (y_mitbih_train!=0))\n",
    "    print(f\"tau={t}: 训练集里 高p0比例，总={np.mean(p0>=t):.4f}, 真0占比={tp:.4f}, 误杀非0占比={fp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d5f95129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.5294 - loss: 1.1015 - val_accuracy: 0.6536 - val_loss: 1.0457 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5832 - loss: 0.9384 - val_accuracy: 0.6520 - val_loss: 0.9395 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5886 - loss: 0.9041 - val_accuracy: 0.6441 - val_loss: 0.9022 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5997 - loss: 0.8734 - val_accuracy: 0.6441 - val_loss: 0.8949 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5921 - loss: 0.8689 - val_accuracy: 0.6242 - val_loss: 0.9086 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6114 - loss: 0.8433 - val_accuracy: 0.6076 - val_loss: 0.9437 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6113 - loss: 0.8379 - val_accuracy: 0.6258 - val_loss: 0.9228 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6085 - loss: 0.8418 - val_accuracy: 0.6378 - val_loss: 0.9064 - learning_rate: 5.0000e-04\n",
      "Epoch 9/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6177 - loss: 0.8351 - val_accuracy: 0.6430 - val_loss: 0.8758 - learning_rate: 5.0000e-04\n",
      "Epoch 10/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6178 - loss: 0.8267 - val_accuracy: 0.6408 - val_loss: 0.8980 - learning_rate: 5.0000e-04\n",
      "Epoch 11/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6124 - loss: 0.8254 - val_accuracy: 0.6408 - val_loss: 0.8796 - learning_rate: 5.0000e-04\n",
      "Epoch 12/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6241 - loss: 0.8222 - val_accuracy: 0.6452 - val_loss: 0.8734 - learning_rate: 5.0000e-04\n",
      "Epoch 13/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6201 - loss: 0.8349 - val_accuracy: 0.6501 - val_loss: 0.8840 - learning_rate: 5.0000e-04\n",
      "Epoch 14/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6260 - loss: 0.8269 - val_accuracy: 0.6375 - val_loss: 0.8773 - learning_rate: 5.0000e-04\n",
      "Epoch 15/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6127 - loss: 0.8237 - val_accuracy: 0.6541 - val_loss: 0.8752 - learning_rate: 5.0000e-04\n",
      "Epoch 16/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6244 - loss: 0.8245 - val_accuracy: 0.6520 - val_loss: 0.8696 - learning_rate: 2.5000e-04\n",
      "Epoch 17/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6245 - loss: 0.8185 - val_accuracy: 0.6563 - val_loss: 0.8667 - learning_rate: 2.5000e-04\n",
      "Epoch 18/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6175 - loss: 0.8193 - val_accuracy: 0.6528 - val_loss: 0.8633 - learning_rate: 2.5000e-04\n",
      "Epoch 19/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6193 - loss: 0.8228 - val_accuracy: 0.6599 - val_loss: 0.8555 - learning_rate: 2.5000e-04\n",
      "Epoch 20/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6245 - loss: 0.8197 - val_accuracy: 0.6520 - val_loss: 0.8641 - learning_rate: 2.5000e-04\n",
      "Epoch 21/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6243 - loss: 0.8168 - val_accuracy: 0.6541 - val_loss: 0.8625 - learning_rate: 2.5000e-04\n",
      "Epoch 22/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6258 - loss: 0.8116 - val_accuracy: 0.6520 - val_loss: 0.8681 - learning_rate: 2.5000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6202 - loss: 0.8155 - val_accuracy: 0.6536 - val_loss: 0.8592 - learning_rate: 1.2500e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6223 - loss: 0.8145 - val_accuracy: 0.6588 - val_loss: 0.8549 - learning_rate: 1.2500e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6246 - loss: 0.8127 - val_accuracy: 0.6574 - val_loss: 0.8576 - learning_rate: 1.2500e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6274 - loss: 0.8096 - val_accuracy: 0.6566 - val_loss: 0.8568 - learning_rate: 1.2500e-04\n",
      "Epoch 27/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6225 - loss: 0.8130 - val_accuracy: 0.6563 - val_loss: 0.8566 - learning_rate: 1.2500e-04\n",
      "Epoch 28/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6222 - loss: 0.8161 - val_accuracy: 0.6566 - val_loss: 0.8532 - learning_rate: 6.2500e-05\n",
      "Epoch 29/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6242 - loss: 0.8110 - val_accuracy: 0.6563 - val_loss: 0.8545 - learning_rate: 6.2500e-05\n",
      "Epoch 30/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6266 - loss: 0.8093 - val_accuracy: 0.6580 - val_loss: 0.8515 - learning_rate: 6.2500e-05\n",
      "Epoch 31/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6272 - loss: 0.8189 - val_accuracy: 0.6590 - val_loss: 0.8500 - learning_rate: 6.2500e-05\n",
      "Epoch 32/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6223 - loss: 0.8146 - val_accuracy: 0.6607 - val_loss: 0.8500 - learning_rate: 6.2500e-05\n",
      "Epoch 33/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6221 - loss: 0.8176 - val_accuracy: 0.6585 - val_loss: 0.8509 - learning_rate: 6.2500e-05\n",
      "Epoch 34/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6231 - loss: 0.8128 - val_accuracy: 0.6582 - val_loss: 0.8516 - learning_rate: 6.2500e-05\n",
      "Epoch 35/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6261 - loss: 0.8208 - val_accuracy: 0.6582 - val_loss: 0.8516 - learning_rate: 3.1250e-05\n",
      "Epoch 36/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6238 - loss: 0.8097 - val_accuracy: 0.6580 - val_loss: 0.8504 - learning_rate: 3.1250e-05\n",
      "Epoch 37/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6275 - loss: 0.8150 - val_accuracy: 0.6590 - val_loss: 0.8495 - learning_rate: 3.1250e-05\n",
      "Epoch 38/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6282 - loss: 0.8162 - val_accuracy: 0.6596 - val_loss: 0.8502 - learning_rate: 3.1250e-05\n",
      "Epoch 39/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6270 - loss: 0.8086 - val_accuracy: 0.6604 - val_loss: 0.8488 - learning_rate: 3.1250e-05\n",
      "Epoch 40/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6264 - loss: 0.8084 - val_accuracy: 0.6593 - val_loss: 0.8490 - learning_rate: 3.1250e-05\n",
      "Epoch 41/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6274 - loss: 0.8118 - val_accuracy: 0.6588 - val_loss: 0.8499 - learning_rate: 3.1250e-05\n",
      "Epoch 42/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6220 - loss: 0.8160 - val_accuracy: 0.6574 - val_loss: 0.8499 - learning_rate: 3.1250e-05\n",
      "Epoch 43/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6263 - loss: 0.8074 - val_accuracy: 0.6588 - val_loss: 0.8489 - learning_rate: 1.5625e-05\n",
      "Epoch 44/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6272 - loss: 0.8171 - val_accuracy: 0.6593 - val_loss: 0.8486 - learning_rate: 1.5625e-05\n",
      "Epoch 45/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6245 - loss: 0.8092 - val_accuracy: 0.6599 - val_loss: 0.8478 - learning_rate: 1.5625e-05\n",
      "Epoch 46/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6278 - loss: 0.8133 - val_accuracy: 0.6590 - val_loss: 0.8492 - learning_rate: 1.5625e-05\n",
      "Epoch 47/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6290 - loss: 0.8093 - val_accuracy: 0.6590 - val_loss: 0.8491 - learning_rate: 1.5625e-05\n",
      "Epoch 48/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6224 - loss: 0.8184 - val_accuracy: 0.6599 - val_loss: 0.8488 - learning_rate: 1.5625e-05\n",
      "Epoch 49/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6253 - loss: 0.8188 - val_accuracy: 0.6593 - val_loss: 0.8485 - learning_rate: 7.8125e-06\n",
      "Epoch 50/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6285 - loss: 0.8165 - val_accuracy: 0.6599 - val_loss: 0.8482 - learning_rate: 7.8125e-06\n",
      "Epoch 51/60\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.6317 - loss: 0.8095 - val_accuracy: 0.6599 - val_loss: 0.8482 - learning_rate: 7.8125e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3f2a28fa0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class_weight（基于筛后训练集）\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cls_w = class_weight.compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(5),\n",
    "    y=y_train_hard\n",
    ")\n",
    "cls_w = {i: w for i, w in enumerate(cls_w)}\n",
    "\n",
    "# 一个稳健的 MLP\n",
    "import tensorflow as tf\n",
    "inputs = tf.keras.Input(shape=(X_train_hard.shape[1],))\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(inputs)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "outputs = tf.keras.layers.Dense(5, activation='softmax')(x)\n",
    "mlp = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "mlp.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "cb = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "]\n",
    "\n",
    "# 假设你已经得到筛后的：\n",
    "# X_train_hard, y_train_hard\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_hard, y_train_hard,\n",
    "    test_size=0.15,           # 15% 做验证\n",
    "    random_state=42,\n",
    "    stratify=y_train_hard     # 保持类别比例\n",
    ")\n",
    "\n",
    "# 然后把 fit 里的变量换成：\n",
    "mlp.fit(\n",
    "    X_tr, y_tr,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=60, batch_size=256,\n",
    "    class_weight=cls_w,\n",
    "    callbacks=cb,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52e2e871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Val accuracy: 0.6599\n",
      "[MLP] Val accuracy(sklearn): 0.6599\n",
      "[MLP] Val classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7233    0.4713    0.5707      1409\n",
      "           1     0.4521    0.7066    0.5514       334\n",
      "           2     0.6712    0.6797    0.6754       868\n",
      "           3     0.1985    0.8333    0.3206        96\n",
      "           4     0.8979    0.8839    0.8909       965\n",
      "\n",
      "    accuracy                         0.6599      3672\n",
      "   macro avg     0.5886    0.7150    0.6018      3672\n",
      "weighted avg     0.7185    0.6599    0.6713      3672\n",
      "\n",
      "[MLP] Val confusion matrix:\n",
      " [[664 254 216 222  53]\n",
      " [ 66 236  17  13   2]\n",
      " [124  28 590  85  41]\n",
      " [  8   2   5  80   1]\n",
      " [ 56   2  51   3 853]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# 1) Keras 自带 evaluate（只看准确率）\n",
    "val_loss, val_acc = mlp.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"[MLP] Val accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# 2) 更详细的指标\n",
    "y_val_pred = np.argmax(mlp.predict(X_val, verbose=0), axis=1)\n",
    "print(f\"[MLP] Val accuracy(sklearn): {accuracy_score(y_val, y_val_pred):.4f}\")\n",
    "print(\"[MLP] Val classification report:\\n\", classification_report(y_val, y_val_pred, digits=4))\n",
    "print(\"[MLP] Val confusion matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d16ee969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cascade@tau=0.95] acc=0.6650  macroF1=0.6076  gated=3/3672 (0.1%)  gate_fp=3 (0.13% of non-0)\n",
      "[Cascade@tau=0.97] acc=0.6650  macroF1=0.6076  gated=3/3672 (0.1%)  gate_fp=3 (0.13% of non-0)\n",
      "[Cascade@tau=0.98] acc=0.6656  macroF1=0.6080  gated=0/3672 (0.0%)  gate_fp=0 (0.00% of non-0)\n",
      "[Cascade@tau=0.99] acc=0.6656  macroF1=0.6080  gated=0/3672 (0.0%)  gate_fp=0 (0.00% of non-0)\n",
      "\n",
      "Best tau on val: 0.98\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7286    0.4649    0.5676      1409\n",
      "           1     0.4547    0.7216    0.5579       334\n",
      "           2     0.6807    0.7097    0.6949       868\n",
      "           3     0.1985    0.8438    0.3214        96\n",
      "           4     0.9151    0.8819    0.8982       965\n",
      "\n",
      "    accuracy                         0.6656      3672\n",
      "   macro avg     0.5955    0.7243    0.6080      3672\n",
      "weighted avg     0.7275    0.6656    0.6772      3672\n",
      "\n",
      "[[655 260 216 229  49]\n",
      " [ 65 241  14  11   3]\n",
      " [120  23 616  83  26]\n",
      " [  7   2   5  81   1]\n",
      " [ 52   4  54   4 851]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# 若你用的是多分类树（classes_=[0 1 2 3 4]），取 p0 的函数\n",
    "idx0 = list(tree.classes_).index(0)\n",
    "def p0_from_tree(tree, X):\n",
    "    return tree.predict_proba(X)[:, idx0]\n",
    "\n",
    "def predict_cascade_on(X, tau=0.98):\n",
    "    p0 = p0_from_tree(tree, X)\n",
    "    gate = (p0 >= tau)               # 高置信 0 → 直接判 0\n",
    "    y_pred = np.empty(X.shape[0], dtype=int)\n",
    "    idx = np.where(~gate)[0]         # 其余交给 MLP 五分类\n",
    "    if len(idx) > 0:\n",
    "        y_pred[idx] = np.argmax(mlp.predict(X[idx], verbose=0), axis=1)\n",
    "    y_pred[gate] = 0\n",
    "    return y_pred, gate\n",
    "\n",
    "taus = [0.95, 0.97, 0.98, 0.99]\n",
    "p0_val = p0_from_tree(tree, X_val)\n",
    "non0_val = np.sum(y_val != 0)\n",
    "\n",
    "best = None\n",
    "for tau in taus:\n",
    "    y_pred_c, gate = predict_cascade_on(X_val, tau=tau)\n",
    "    acc = accuracy_score(y_val, y_pred_c)\n",
    "    macro_f1 = f1_score(y_val, y_pred_c, average='macro')\n",
    "    gate_fp = np.sum(gate & (y_val != 0))  # 被树误杀的非0\n",
    "    print(f\"[Cascade@tau={tau}] acc={acc:.4f}  macroF1={macro_f1:.4f}  \"\n",
    "          f\"gated={gate.sum()}/{len(y_val)} ({gate.mean():.1%})  \"\n",
    "          f\"gate_fp={gate_fp} ({gate_fp/non0_val:.2%} of non-0)\")\n",
    "    score = (macro_f1, acc)  # 先看宏F1，再看acc\n",
    "    if (best is None) or (score > best[0]):\n",
    "        best = (score, tau, y_pred_c)\n",
    "\n",
    "best_tau = best[1]\n",
    "print(\"\\nBest tau on val:\", best_tau)\n",
    "print(classification_report(y_val, best[2], digits=4))\n",
    "print(confusion_matrix(y_val, best[2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
